[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESS 330 lab 6",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggplot2)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\nQuestion 1\nzero_q_freq represents frequency of days with Q = 0 mm/day (%), where Q= discharge (mm/day).\nDemo–&gt;\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nlibrary(baguette)\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.592\n2 rsq     standard       0.736\n3 mae     standard       0.367\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\nDemo –^\n\n\nQuestion 2\n\nmap_1 &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) + \n  scale_color_gradient(low = \"lightblue\", high = \"orange\") +\n  coord_fixed() +\n  labs(title = \"Aridity across the U.S\",\n       x = \"Longitude\", y = \"Latitude\", color = \"Aridity (Priestley-Taylor formulation)\") +\n    ggthemes::theme_map() +\n   theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\")\n\n\nmap_2 &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) + \n  scale_color_gradient(low = \"pink\", high = \"lightblue\") +\n  coord_fixed() +\n  labs(title = \"Mean Daily Preciptiation Across the U.S\",\n       x = \"Longitude\", y = \"Latitude\", color = \"Mean Precipitation (mm/day)\") +\n    ggthemes::theme_map() +\n   theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\")\n\n\nlibrary(patchwork)\nmap_1 + map_2\n\n\n\n\n\n\n\n\n\n\nQuestion 3\nDefine additional models\n\n# XGBoost\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Bagged MLP Neural Net\nmlp_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nCreate workflows\n\n# XGBoost Workflow\nxgb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_model) %&gt;%\n  fit(data = camels_train)\n\n# Bagged Neural Net Workflow\nmlp_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(mlp_model) %&gt;%\n  fit(data = camels_train)\n\nPredictions\n\n# Predict and add to test set\nlm_data &lt;- camels_test %&gt;%\n  mutate(.pred = predict(lm_wf, .)$.pred)\n\nrf_data &lt;- camels_test %&gt;%\n  mutate(.pred = predict(rf_wf, .)$.pred)\n\nxgb_data &lt;- camels_test %&gt;%\n  mutate(.pred = predict(xgb_wf, .)$.pred)\n\nmlp_data &lt;- camels_test %&gt;%\n  mutate(.pred = predict(mlp_wf, .)$.pred)\n\nEvaluate metrics\n\nmodel_metrics &lt;- bind_rows(\n  metrics(lm_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Linear\"),\n  metrics(rf_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Random Forest\"),\n  metrics(xgb_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"XGBoost\"),\n  metrics(mlp_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Bagged MLP\")\n) %&gt;%\n  select(model, everything())\nprint(model_metrics)\n\n# A tibble: 12 × 4\n   model         .metric .estimator .estimate\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n 1 Linear        rmse    standard       0.583\n 2 Linear        rsq     standard       0.742\n 3 Linear        mae     standard       0.390\n 4 Random Forest rmse    standard       0.592\n 5 Random Forest rsq     standard       0.736\n 6 Random Forest mae     standard       0.367\n 7 XGBoost       rmse    standard       0.631\n 8 XGBoost       rsq     standard       0.702\n 9 XGBoost       mae     standard       0.397\n10 Bagged MLP    rmse    standard       0.559\n11 Bagged MLP    rsq     standard       0.761\n12 Bagged MLP    mae     standard       0.348\n\n\nVisualize XGB\n\nxgb_preds &lt;- camels_test %&gt;%\n  mutate(.pred = predict(xgb_wf, .)$.pred)\n\nggplot(xgb_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  scale_color_viridis_c() +\n  geom_point(alpha = 0.6) +\n  geom_abline(linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    title = \"XGBoost: Observed vs Predicted\",\n    x = \"Observed logQmean\",\n    y = \"Predicted logQmean\"\n  )\n\n\n\n\n\n\n\n\nVisualize MLP\n\nmlp_preds &lt;- camels_test %&gt;%\n  mutate(.pred = predict(mlp_wf, .)$.pred)\n\nggplot(mlp_preds, aes(x = logQmean, y = .pred, color = aridity)) +\n  scale_color_viridis_c() +\n  geom_point(alpha = 0.6) +\n  geom_abline(linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    title = \"Bagged MLP: Observed vs Predicted\",\n    x = \"Observed logQmean\",\n    y = \"Predicted logQmean\"\n  )\n\n\n\n\n\n\n\n\nFor modeling this data, I would chose the Bagged MLP model as it preforms the best. It has the lowest RMSE and MAE, which indicates better accuracy. It also has the highest R^2 which indicated better correlations.\n\n\nQuestion 4\n\n# Set seed for reproducibility\nset.seed(1102)\n\n#removing NAs \ncamels |&gt; \n  select(slope_mean, p_mean, logQmean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           slope_mean    p_mean  logQmean\nslope_mean  1.0000000 0.2602688 0.3687933\np_mean      0.2602688 1.0000000 0.8053056\nlogQmean    0.3687933 0.8053056 1.0000000\n\n# Initial train/test split\nsplit &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(split)\ncamels_test  &lt;- testing(split)\n\n# Create 10-fold cross-validation\ncv_folds &lt;- vfold_cv(camels_train, v = 10)\n\n# Defining my recipe\nrec2 &lt;- recipe(logQmean ~ p_mean + slope_mean  , data = camels_train) %&gt;%\n  step_log(all_predictors())%&gt;%\n  step_interact(terms = ~ p_mean:slope_mean) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())               \n\nFor this formula I chose slope_mean and p_mean. Slope_mean is the mean slope of the catchment. I assumed this would impact discharge because the water is more likely to run off and down the catchment due to gravity, and I suspect that all other variables held constant, and area with a greater slope would have greater daily discharge. I also chose p_mean which is mean daily precipitation, which would influence discharge because more water in the area means more area in the river/stream. I chose to add these together because greater values of each variable would compound into greater discharge.\n\n#Bake Data\nbaked_data2 &lt;- prep(rec2, camels_train) %&gt;%\n  bake(new_data = NULL)\n\n#linear model\nlm_base2 &lt;- lm(logQmean ~ p_mean * slope_mean, data = baked_data2)\nsummary(lm_base2)\n\n\nCall:\nlm(formula = logQmean ~ p_mean * slope_mean, data = baked_data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.59234 -0.26155  0.02523  0.25449  1.65766 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -4.77993    0.17535 -27.260   &lt;2e-16 ***\np_mean             3.67249    0.15618  23.515   &lt;2e-16 ***\nslope_mean         0.62593    0.04691  13.344   &lt;2e-16 ***\np_mean:slope_mean -0.37395    0.04010  -9.326   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4877 on 498 degrees of freedom\nMultiple R-squared:  0.8372,    Adjusted R-squared:  0.8362 \nF-statistic: 853.6 on 3 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nI could not find variables with an R^2 &gt;0.9, but this once is high and about the closest I found\nDefine Models\n\nxgb_model2 &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nrf_model2 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nlm_model2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")%&gt;%\n  set_mode(\"regression\")\n\nCreating workflow\n\n# Linear regression workflow\n\nlm_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec2) %&gt;%\n  add_model(lm_model2)\n\n# Random forest workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec2) %&gt;%\n  add_model(rf_model2)\n\n# XGBoost workflow\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec2) %&gt;%\n  add_model(xgb_model2)\n\nFit to resamples\n\n# Fit linear model\nlm_res &lt;- fit_resamples(\n  lm_workflow,\n  resamples = cv_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Fit random forest\nrf_res &lt;- fit_resamples(\n  rf_workflow,\n  resamples = cv_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Fit XGBoost\nxgb_res &lt;- fit_resamples(\n  xgb_workflow,\n  resamples = cv_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nCreate workflow set\n\n# Create a workflow set\nmodel_set &lt;- workflow_set(\n  preproc = list(rec2 = rec2),\n  models = list(\n    linear_reg = lm_model2,\n    random_forest = rf_model2,\n    xgboost = xgb_model2\n  )\n)\n\n# Tune all models in the workflow set\nmodel_set_res &lt;- model_set %&gt;%\n  workflow_map(\"tune_grid\", resamples = cv_folds)\n\nRank results\n\n# Rank based on RMSE\nrank_results(model_set_res, rank_metric = \"rmse\")\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 rec2_random_fore… Prepro… rmse    0.438  0.0239    10 recipe       rand…     1\n2 rec2_random_fore… Prepro… rsq     0.875  0.0113    10 recipe       rand…     1\n3 rec2_xgboost      Prepro… rmse    0.470  0.0237    10 recipe       boos…     2\n4 rec2_xgboost      Prepro… rsq     0.861  0.0117    10 recipe       boos…     2\n5 rec2_linear_reg   Prepro… rmse    0.485  0.0251    10 recipe       line…     3\n6 rec2_linear_reg   Prepro… rsq     0.842  0.0116    10 recipe       line…     3\n\n\nPlot results\n\nautoplot(model_set_res)\n\n\n\n\n\n\n\n\nThe model that I think is best is the Random Forest, as it has the lowest RMSE, indicating best prediction accuracy on average. it also has the highest R^2 and a low standard error, which indicates that this model explains the most variance in streamflow, nad is also relatively stable across the 10 reseamples.\nExtract and evaluate\n\nfinal_rf_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec2) %&gt;%\n  add_model(rf_model2)\n\nfinal_rf_fit &lt;- final_rf_workflow %&gt;%\n  fit(data = camels_train)\n\nrf_predictions &lt;- augment(final_rf_fit, new_data = camels_test)\n\nggplot(rf_predictions, aes(x = .pred, y = logQmean)) + \n  geom_point(aes(color = slope_mean ), alpha = 0.7) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Observed vs. Predicted Mean Streamflow\",\n    x = \"Predicted Streamflow (logQmean)\",\n    y = \"Observed Streamflow (logQmean)\",\n    color = \"Mean Slope (m/km)\"\n  ) +\n  scale_color_viridis_c() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn these results I see that where the stream flow is low, it is harder to predict based on my defined variables. This makes sense because I know that when streams or areas are dry, any precipitation can cause them to become “flashy”. Where the stream flow is positive, the observed and predicted stream flow are much closer together and the fit seems good. I can also see that there is a good trend in slope and flow, where more slope creates more observed streamlow, as I predicted."
  }
]